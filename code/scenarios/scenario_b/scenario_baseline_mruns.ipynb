{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario B - Baseline Variation (multiple runs)\n",
    "\n",
    "In this scenario the baseline underlying a spectrum with a fixed number of Gaussian peaks is varied between no baseline, a constant y-offset and a linear baseline. All datasets contain 3 peaks and the noise level is kept constant at 1%.\n",
    "\n",
    "The models used in the inference of the parameters are formulated as follows:\n",
    "\n",
    "1. No baseline:\n",
    "\\begin{equation}\n",
    "\\large y = f(x) = \\sum\\limits_{m=1}^M \\big[A_m \\cdot e^{-\\frac{(x-\\mu_m)^2}{2\\cdot\\sigma_m^2}}\\big] + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "2. Constant offset:\n",
    "\\begin{equation}\n",
    "\\large y = f(x) = \\sum\\limits_{m=1}^M \\big[A_m \\cdot e^{-\\frac{(x-\\mu_m)^2}{2\\cdot\\sigma_m^2}}\\big] + a_0 + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "3. Linear baseline:\n",
    "\\begin{equation}\n",
    "\\large y = f(x) = \\sum\\limits_{m=1}^M \\big[A_m \\cdot e^{-\\frac{(x-\\mu_m)^2}{2\\cdot\\sigma_m^2}}\\big] + a_0 + a_1 \\cdot x + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "This file runs a series of inference runs for a set of generated spectra. New spectra are generated for each run and stored. After running inference, only the summary statistics are stored and the next run is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "\n",
    "#az.style.use('arviz-darkgrid')\n",
    "\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../modules')\n",
    "import datagen as dg\n",
    "import models as mdl\n",
    "import results as res\n",
    "import figures as fig\n",
    "import settings as cnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for results and images\n",
    "out_path      = './output_mruns'\n",
    "file_basename = out_path + '/scenario_baseline'\n",
    "\n",
    "conf = {}\n",
    "    \n",
    "# scenario name\n",
    "conf['scenario'] = 'peak variation'\n",
    "    \n",
    "# initialization method for sampler\n",
    "conf['init_mode'] = 'adapt_diag'\n",
    "\n",
    "# probabilistic model (priors)\n",
    "conf['prior_model'] = 'lognormal'\n",
    "\n",
    "# provide peak positions to the model as testvalues ('yes'/'no')\n",
    "conf['peak_info'] = 'yes'\n",
    "\n",
    "# data mode ('generate'/'preload')\n",
    "conf['data_mode'] = 'generate'\n",
    "\n",
    "# number of runs\n",
    "conf['nruns'] = 4\n",
    "\n",
    "# number of cores to run sampling chains on\n",
    "conf['ncores'] = 2\n",
    "\n",
    "# number of samples per chain\n",
    "conf['nsamples'] = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the output dir does not exist, create it\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf.save(out_path, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list of wavelengths (x-values)\n",
    "xval = [i for i in range(200, 400, 2)]\n",
    "\n",
    "# number of spectra per peak number\n",
    "nsets  = 4\n",
    "\n",
    "# baseline variation\n",
    "baselines = ['none', 'offset', 'linear']\n",
    "\n",
    "lbline = [bl for bl in baselines for i in range(nsets)]\n",
    "\n",
    "# total number of datasets\n",
    "tsets = nsets * len(baselines)\n",
    "\n",
    "# total number of inference runs (per run)\n",
    "truns = nsets * len(baselines)**2\n",
    "\n",
    "# generate nruns sets of spectra\n",
    "for r in range(conf['nruns']):\n",
    "    print(\"Generating dataset {0} of {1}\".format(r+1,conf['nruns']))\n",
    "    \n",
    "    ldata, lpeaks = [], []\n",
    "    \n",
    "    # create output directory for data\n",
    "    out_dir = out_path + '/run_{0:02d}'.format(r+1)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "            \n",
    "    for blv in baselines:\n",
    "        for i in range(nsets):\n",
    "            df, peaks, _ = dg.data_generator(xvalues=xval, nsamples=15, npeaks=3, tbaseline=blv)\n",
    "            ldata.append(df)\n",
    "            lpeaks.append(peaks)\n",
    "            \n",
    "    # save data and peak information to disk\n",
    "    for i in range(len(ldata)):\n",
    "        ldata[i].to_csv(out_dir + '/dataset_{0:02d}.csv'.format(i+1), index=False)\n",
    "    dg.data_save(out_dir + '/peakinfo.csv', lpeaks)\n",
    "    \n",
    "    # plot datasets\n",
    "    filen = out_dir + '/scenario_baseline'\n",
    "    fig.plot_datasets(ldata, lpeaks, dims=(int(tsets/2),2), figure_size=(12,int(tsets*(1.8))), \n",
    "                                            savefig='yes', fname=filen, scenario='baseline', labels=lbline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of multiple runs                    : {0}\".format(conf['nruns']))\n",
    "print(\"total number of baseline variations              : {0}\".format(len(baselines)))\n",
    "print(\"total number of datasets per baseline variation  : {0}\".format(nsets))\n",
    "print(\"total number of datasets per model               : {0}\".format(tsets))\n",
    "print(\"total number of inference runs (per single loop) : {0}\".format(truns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, run inference, visualize, collect results and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas data to numpy arrays\n",
    "x_val = np.array(xval, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe to hold multiple run results\n",
    "res_df = pd.DataFrame()\n",
    "\n",
    "# run the whole loop of inference, posterior sampling, results collection and saving\n",
    "for r in range(conf['nruns']):\n",
    "    print(\"starting loop {0}/{1}\".format(r+1,conf['nruns']))\n",
    "\n",
    "    models, traces, lmodbase = [], [], []\n",
    "\n",
    "    # load datasets from disk\n",
    "    data_dir = out_path + '/run_{0:02d}'.format(r+1)\n",
    "    ldata, lpeaks, _ = dg.data_load(tsets, data_dir)\n",
    "\n",
    "    # store dataset y-values in list\n",
    "    cols = ldata[0].columns\n",
    "    y_val = [ldata[i][cols].values for i in range(len(ldata))]\n",
    "\n",
    "    # actual inference run number\n",
    "    inf_run = 1\n",
    "\n",
    "    for bl in baselines:\n",
    "        print(\"running: baseline-{0} model\".format(bl))\n",
    "        for i in range(len(ldata)):\n",
    "            if conf['peak_info'] == 'yes':\n",
    "                plist = np.array(lpeaks[i], dtype=float).flatten()\n",
    "                plist.sort()\n",
    "                model_g = mdl.model_pvoigt(xvalues=x_val, observations=y_val[i], npeaks=3, \n",
    "                                      mu_peaks=plist, pmodel=conf['prior_model'], baseline=bl)\n",
    "            else:\n",
    "                model_g = mdl.model_pvoigt(xvalues=x_val, observations=y_val[i], npeaks=3,\n",
    "                                                      pmodel=conf['prior_model'], baseline=bl)  \n",
    "            models.append(model_g)\n",
    "\n",
    "            with model_g:\n",
    "                print(\"({6}:{2}/{3}) running inference on dataset #{0}/{1} [{4} model: {5} data]\"\n",
    "                      .format(i+1,len(ldata),inf_run,truns,bl,lbline[i],r+1))\n",
    "                trace_g = pm.sample(conf['nsamples'], init=conf['init_mode'], cores=conf['ncores'])\n",
    "                lmodbase += [(bl,lbline[i])]\n",
    "                traces.append(trace_g)\n",
    "                inf_run += 1\n",
    "\n",
    "    # save model figure as image (once), foreach model\n",
    "    if r == 0:\n",
    "        z = 0\n",
    "        for i in range(len(models)):\n",
    "            if i % len(lbline) == 0:\n",
    "                img = pm.model_to_graphviz(models[i])\n",
    "                img.render(filename=file_basename + '_model_{0}'.format(baselines[z]), format='png');\n",
    "                z += 1\n",
    "\n",
    "    # posterior predictive traces\n",
    "    ppc = [pm.sample_posterior_predictive(traces[i], samples=500, model=models[i]) for i in range(len(traces))]\n",
    "\n",
    "    # collect the results, concat single run result to overall result \n",
    "    lruns = ['{0}'.format(r+1) for i in range(truns)]\n",
    "    df = res.get_results_summary(traces, ppc, y_val, epsilon_real=0.05, sets=tsets, \n",
    "                                     labels=lmodbase, multimodels='yes', scenario='baseline', runlist=lruns)\n",
    "    \n",
    "    res_df = res_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to .csv\n",
    "res_df.to_csv(out_path + '/scenario_baseline_mruns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf.close(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
